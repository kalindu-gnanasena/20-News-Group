{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "\n",
    "# libraries importing from scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# natural language toolkit packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n",
    "from nltk.stem.snowball import  EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the set of stop words\n",
    "_stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "# initialize treebank word tokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# initialize treebank word detokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "# initialize english stemmer\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "# initialize word net lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# word list used in english language\n",
    "_word_list = set([word for word in wn.words(lang='eng')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get all the text files from the directory to a list\n",
    "def read_files(dir):\n",
    "    f = []\n",
    "    for roots, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            f.append(os.path.join(roots, file))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to categorize and read the files\n",
    "def open_files(file_paths):\n",
    "    comp = []\n",
    "    rec = []\n",
    "    sci = []\n",
    "    talk = []\n",
    "    other = []\n",
    "    \n",
    "    for path in file_paths:\n",
    "        if \"comp\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            comp += [w.read()]\n",
    "        elif \"rec\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            rec += [w.read()]\n",
    "        elif \"sci\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            sci += [w.read()]\n",
    "        elif \"talk\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            talk += [w.read()]\n",
    "        elif \"alt\" in path or \"misc\" in path or \"soc\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            other += [w.read()]\n",
    "    return comp, rec, sci, talk, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to clean data\n",
    "def clean_data(data):\n",
    "    cleaned_data = []\n",
    "    for text in data:\n",
    "        words = []\n",
    "        for word in tokenizer.tokenize(text):\n",
    "            word = stemmer.stem(word)\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            words += [word]   \n",
    "        cleaned_data += [detokenizer.detokenize([word for word in words if word in _word_list if word.isalpha()])]\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize_csv(cleaned_data, name):\n",
    "    vectorizer = CountVectorizer(stop_words=_stop_words)\n",
    "    x = vectorizer.fit_transform(cleaned_data)\n",
    "    y = vectorizer.get_feature_names()\n",
    "    \n",
    "    df = pd.DataFrame(data=x.toarray(), columns=y)\n",
    "    \n",
    "    train, test = train_test_split(df, train_size=0.7, test_size=0.3, random_state=42)\n",
    "    \n",
    "    train.to_csv(name+'_count_train.csv')\n",
    "    test.to_csv(name+'_count_test.csv')\n",
    "    \n",
    "    print('Count vectorize train and test csv for {0} is successfully created!'.format(name))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_vectorize_csv(cleaned_data, name):\n",
    "    vectorizer = CountVectorizer(stop_words=_stop_words, binary=True)\n",
    "    x = vectorizer.fit_transform(cleaned_data)\n",
    "    y = vectorizer.get_feature_names()\n",
    "    \n",
    "    df = pd.DataFrame(data=x.toarray(), columns=y)\n",
    "    \n",
    "    train, test = train_test_split(df, train_size=0.7, test_size=0.3, random_state=42)\n",
    "    \n",
    "    train.to_csv(name+'_binary_train.csv')\n",
    "    test.to_csv(name+'_binary_test.csv')\n",
    "    \n",
    "    print('Binary vectorize train and test csv for {0} is successfully created!'.format(name))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorize_csv(cleaned_data, name):\n",
    "    vectorizer = TfidfVectorizer(stop_words=_stop_words)\n",
    "    x = vectorizer.fit_transform(cleaned_data)\n",
    "    y = vectorizer.get_feature_names()\n",
    "    \n",
    "    df = pd.DataFrame(data=x.toarray(), columns=y)\n",
    "    \n",
    "    train, test = train_test_split(df, train_size=0.7, test_size=0.3, random_state=42)\n",
    "    \n",
    "    train.to_csv(name+'_tfidf_train.csv')\n",
    "    test.to_csv(name+'_tfidf_test.csv')\n",
    "    \n",
    "    print('Tfidf vectorize train and test csv for {0} is successfully created!'.format(name))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vectors(data, name):\n",
    "    cleaned_data = clean_data(data)\n",
    "    \n",
    "    c_train, c_test = count_vectorize_csv(cleaned_data, name)\n",
    "    b_train, b_test = binary_vectorize_csv(cleaned_data, name)\n",
    "    t_train, t_test = tfidf_vectorize_csv(cleaned_data, name)\n",
    "    return c_train, c_test, b_train, b_test, t_train, t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = read_files('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = []\n",
    "rec_data = []\n",
    "sci_data = []\n",
    "talk_data = []\n",
    "other_data = []\n",
    "\n",
    "comp_data, rec_data, sci_data, talk_data, other_data = open_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorize train and test csv for comp is successfully created!\n",
      "Binary vectorize train and test csv for comp is successfully created!\n",
      "Tfidf vectorize train and test csv for comp is successfully created!\n"
     ]
    }
   ],
   "source": [
    "comp_c_train, comp_c_test, comp_b_train, comp_b_test, comp_t_train, comp_t_test = get_feature_vectors(comp_data, 'comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorize train and test csv for rec is successfully created!\n",
      "Binary vectorize train and test csv for rec is successfully created!\n",
      "Tfidf vectorize train and test csv for rec is successfully created!\n"
     ]
    }
   ],
   "source": [
    "rec_c_train, rec_c_test, rec_b_train, rec_b_test, rec_t_train, rec_t_test = get_feature_vectors(rec_data, 'rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorize train and test csv for sci is successfully created!\n",
      "Binary vectorize train and test csv for sci is successfully created!\n",
      "Tfidf vectorize train and test csv for sci is successfully created!\n"
     ]
    }
   ],
   "source": [
    "sci_c_train, sci_c_test, sci_b_train, sci_b_test, sci_t_train, sci_t_test = get_feature_vectors(sci_data, 'sci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorize train and test csv for talk is successfully created!\n",
      "Binary vectorize train and test csv for talk is successfully created!\n",
      "Tfidf vectorize train and test csv for talk is successfully created!\n"
     ]
    }
   ],
   "source": [
    "talk_c_train, talk_c_test, talk_b_train, talk_b_test, talk_t_train, talk_t_test = get_feature_vectors(talk_data, 'talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorize train and test csv for other is successfully created!\n",
      "Binary vectorize train and test csv for other is successfully created!\n",
      "Tfidf vectorize train and test csv for other is successfully created!\n"
     ]
    }
   ],
   "source": [
    "other_c_train, other_c_test, other_b_train, other_b_test, other_t_train, other_t_test = get_feature_vectors(other_data, 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
