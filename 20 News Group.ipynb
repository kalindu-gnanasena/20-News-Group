{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 News Group Feature Extraction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "**This dataset is a collection of  20 newsgroups documents. The processing has been done for the purpose of feature extraction.**\n",
    "\n",
    "*This is a list of the 20 newsgroups:*\n",
    "\n",
    "- comp.graphics\n",
    "- comp.os.ms-windows.misc\n",
    "- comp.sys.ibm.pc.hardware\n",
    "- comp.sys.mac.hardware\n",
    "- comp.windows.x rec.autos\n",
    "- rec.motorcycles\n",
    "- rec.sport.baseball\n",
    "- rec.sport.hockey sci.crypt\n",
    "- sci.electronics\n",
    "- sci.med\n",
    "- sci.space\n",
    "- misc.forsale talk.politics.misc\n",
    "- talk.politics.guns\n",
    "- talk.politics.mideast talk.religion.misc\n",
    "- alt.atheism\n",
    "- soc.religion.christian\n",
    "\n",
    "#### Download Link: [20news-bydate.tar.gz ](http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz) - 20 Newsgroups sorted by date; duplicates and some headers removed (18846 documents)\n",
    "\n",
    "The 20 newsgroup dataset was transformed by using the **Bag of word** and **Term frequency-Inverse document frequency (tf-idf)** method. The dataset after transformation consists of five main classes:\n",
    "    \n",
    " - Computer\n",
    " - Recreational\n",
    " - Science\n",
    " - Talkshow\n",
    " \n",
    " and each of these classes contains **`train.csv`** and **`test.csv`** files.\n",
    " \n",
    " \n",
    " **Used Methods**\n",
    " \n",
    " - [x] Bag of words - CountVectorizer\n",
    " - [x] Bag of Words - BinaryVectorizer\n",
    " - [x] TF-IDF - TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Generated CSV Files:\n",
    "\n",
    "`comp_count_train.csv` `comp_count_test.csv` `comp_binary_train.csv` `comp_binary_test.csv` `comp_tfidf_train.csv` `comp_tfidf_test.csv`\n",
    "\n",
    "`rec_count_train.csv` `rec_count_test.csv` `rec_binary_train.csv` `rec_binary_test.csv` `rec_tfidf_train.csv` `rec_tfidf_test.csv`\n",
    "\n",
    "`sci_count_train.csv` `sci_count_test.csv` `sci_binary_train.csv` `sci_binary_test.csv` `sci_tfidf_train.csv` `sci_tfidf_test.csv`\n",
    "\n",
    "`talk_count_train.csv` `talk_count_test.csv` `talk_binary_train.csv` `talk_binary_test.csv` `talk_tfidf_train.csv` `talk_tfidf_test.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules\n",
    "\n",
    "import all the required modules from `os` `pandas` `sklearn` `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "\n",
    "# libraries importing from scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# natural language toolkit packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n",
    "from nltk.stem.snowball import  EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the set of stop words\n",
    "_stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "# initialize treebank word tokenizer and detokenizer\n",
    "tokenizer = TreebankWordTokenizer() \n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "# initialize english stemmer\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "# initialize word net lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# word list used in english language\n",
    "_word_list = set([word for word in wn.words(lang='eng')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the file list\n",
    "\n",
    "Read all the files, directories, roots using `os.walk()` and **return all the files** in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get all the text files from the directory to a list\n",
    "def read_files(dir):\n",
    "    f = []\n",
    "    for roots, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            f.append(os.path.join(roots, file))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and Categorize files\n",
    "\n",
    "Open and read all the files and categorize them into `comp` `rec` `sci` `talk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to categorize and read the files\n",
    "def open_files(file_paths):\n",
    "    comp = []\n",
    "    rec = []\n",
    "    sci = []\n",
    "    talk = []\n",
    "    \n",
    "    for path in file_paths:\n",
    "        if \"comp\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            comp += [w.read()]\n",
    "        elif \"rec\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            rec += [w.read()]\n",
    "        elif \"sci\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            sci += [w.read()]\n",
    "        elif \"talk\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            talk += [w.read()]\n",
    "    return comp, rec, sci, talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of unique set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_word_set(data_list):\n",
    "    word_set = []\n",
    "    for docs in data_list:\n",
    "        for doc in docs:\n",
    "            words = []\n",
    "            for word in word_tokenize(doc):\n",
    "                word = stemmer.stem(word)\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "                words += [word]\n",
    "            word_set += [word for word in words if word not in _stop_words if word in _word_list if word.isalpha()]\n",
    "    return list(set(word_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Preprocess Data\n",
    "\n",
    "**Tokenize** and **normalize** the text and **detokenize** cleaned text set. `stemmer` and `lemmatizer` helps to normalize the texts such as,\n",
    "\n",
    ">running, runs -> run\n",
    " eating, eats -> eat\n",
    "\n",
    "cleaned data without numbers and only valid text words in english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to clean data\n",
    "def clean_data(data):\n",
    "    cleaned_data = []\n",
    "    for text in data:\n",
    "        words = []\n",
    "        for word in tokenizer.tokenize(text):\n",
    "            word = stemmer.stem(word)\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            words += [word]   \n",
    "        cleaned_data += [detokenizer.detokenize([word for word in words if word in _word_list if word.isalpha()])]\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize data\n",
    "---\n",
    "\n",
    "#### Convert a collection of text documents to a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize_csv(cleaned_data, name, vocabulary_set):\n",
    "    vectorizer = CountVectorizer(stop_words=_stop_words, vocabulary=vocabulary_set)\n",
    "    x = vectorizer.fit_transform(cleaned_data)\n",
    "    y = vectorizer.get_feature_names()\n",
    "    \n",
    "    df = pd.DataFrame(data=x.toarray(), columns=y)\n",
    "    \n",
    "    df.to_csv(name+'_freq.csv')\n",
    "    \n",
    "    print(name+'_freq.csv successfully created!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert a collection of text documents to a matrix of binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_vectorize_csv(cleaned_data, name, vocabulary_set):\n",
    "    vectorizer = CountVectorizer(stop_words=_stop_words, binary=True, vocabulary=vocabulary_set)\n",
    "    x = vectorizer.fit_transform(cleaned_data)\n",
    "    y = vectorizer.get_feature_names()\n",
    "    \n",
    "    df = pd.DataFrame(data=x.toarray(), columns=y)\n",
    "    \n",
    "    df.to_csv(name+'_binary.csv')\n",
    "    \n",
    "    print(name+'_binary.csv successfully created!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert a collection of text documents to a matrix of token weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorize_csv(cleaned_data, name, vocabulary_set):\n",
    "    vectorizer = TfidfVectorizer(stop_words=_stop_words, vocabulary=vocabulary_set)\n",
    "    x = vectorizer.fit_transform(cleaned_data)\n",
    "    y = vectorizer.get_feature_names()\n",
    "    \n",
    "    df = pd.DataFrame(data=x.toarray(), columns=y)\n",
    "    \n",
    "    df.to_csv(name+'_tfidf.csv')\n",
    "    \n",
    "    print(name+'_tfidf.csv successfully created!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize data using `CountVectorizer()` and `TfidfVectorizer()` and return `train/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vectors(data, name, vocabulary_set):\n",
    "    cleaned_data = clean_data(data)\n",
    "    \n",
    "    c_train = count_vectorize_csv(cleaned_data, name, vocabulary_set)\n",
    "    b_train = binary_vectorize_csv(cleaned_data, name, vocabulary_set)\n",
    "    t_train = tfidf_vectorize_csv(cleaned_data, name, vocabulary_set)\n",
    "    return c_train, b_train, t_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program Execution\n",
    "\n",
    "---\n",
    "\n",
    "#### Call `read_files()` method to get all the files to the `files` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = read_files('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call `open_file()` method to *read and categorize files* into\n",
    "\n",
    "\n",
    "`comp_data` `rec_data` `sci_data` `talk_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = []\n",
    "rec_data = []\n",
    "sci_data = []\n",
    "talk_data = []\n",
    "\n",
    "comp_data, rec_data, sci_data, talk_data = open_files(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train and test set\n",
    "\n",
    "call `train_test_split()` on the data.\n",
    "\n",
    "**Train data = 70%\n",
    "Test data = 30%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data_train, comp_data_test = train_test_split(comp_data, train_size=0.7, test_size=0.3, random_state=42)\n",
    "rec_data_train, rec_data_test = train_test_split(rec_data, train_size=0.7, test_size=0.3, random_state=42)\n",
    "sci_data_train, sci_data_test = train_test_split(sci_data, train_size=0.7, test_size=0.3, random_state=42)\n",
    "talk_data_train, talk_data_test = train_test_split(talk_data, train_size=0.7, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and Test data sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [comp_data_train, rec_data_train, sci_data_train, talk_data_train]\n",
    "test_data = [comp_data_test, rec_data_test, sci_data_test, talk_data_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get unique word set\n",
    "\n",
    "Call `get_unique_word_set()` to get word dictionary from `comp_train_data` `rec_train_data` `sci_train_data` `talk_train_data`\n",
    "\n",
    "**We use this word dictionary as the `vocabulary` attribute in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_set = get_unique_word_set(train_data)\n",
    "unique_word_set.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call `get_feature_vector` method to `comp_data_train` and `comp_data_test`\n",
    "\n",
    "It generates `train.csv` and `test.csv` files to **computer category**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize `comp_data_train`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp_train_freq.csv successfully created!\n",
      "comp_train_binary.csv successfully created!\n",
      "comp_train_tfidf.csv successfully created!\n"
     ]
    }
   ],
   "source": [
    "comp_f_train, comp_b_train, comp_t_train = get_feature_vectors(comp_data_train, 'comp_train', unique_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize `comp_data_test`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp_test_freq.csv successfully created!\n",
      "comp_test_binary.csv successfully created!\n",
      "comp_test_tfidf.csv successfully created!\n"
     ]
    }
   ],
   "source": [
    "comp_f_test, comp_b_test, comp_t_test = get_feature_vectors(comp_data_test, 'comp_test', unique_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call `get_feature_vector` method to `rec_data_train` and `rec_data_test`\n",
    "\n",
    "It generates `train.csv` and `test.csv` files to **recreation category**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize `rec_data_train`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_train_freq.csv successfully created!\n",
      "rec_train_binary.csv successfully created!\n",
      "rec_train_tfidf.csv successfully created!\n"
     ]
    }
   ],
   "source": [
    "rec_f_train, rec_b_train, rec_t_train = get_feature_vectors(rec_data_train, 'rec_train', unique_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize `rec_data_test`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_test_freq.csv successfully created!\n",
      "rec_test_binary.csv successfully created!\n",
      "rec_test_tfidf.csv successfully created!\n"
     ]
    }
   ],
   "source": [
    "rec_f_test, rec_b_test, rec_t_test = get_feature_vectors(rec_data_test, 'rec_test', unique_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call `get_feature_vector` method to `sci_data_train` and `sci_data_test`\n",
    "\n",
    "It generates `train.csv` and `test.csv` files to **science category**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize `sci_data_train`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci_train_freq.csv successfully created!\n",
      "sci_train_binary.csv successfully created!\n",
      "sci_train_tfidf.csv successfully created!\n"
     ]
    }
   ],
   "source": [
    "sci_f_train, sci_b_train, sci_t_train = get_feature_vectors(sci_data_train, 'sci_train', unique_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize `sci_data_test`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci_test_freq.csv successfully created!\n",
      "sci_test_binary.csv successfully created!\n",
      "sci_test_tfidf.csv successfully created!\n"
     ]
    }
   ],
   "source": [
    "sci_f_test, sci_b_test, sci_t_test = get_feature_vectors(sci_data_test, 'sci_test', unique_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call `get_feature_vector` method to `talk_data_train` and `talk_data_test`\n",
    "\n",
    "It generates `train.csv` and `test.csv` files to **talk show category**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize `talk_data_train`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk_train_freq.csv successfully created!\n",
      "talk_train_binary.csv successfully created!\n",
      "talk_train_tfidf.csv successfully created!\n"
     ]
    }
   ],
   "source": [
    "talk_f_train, talk_b_train, talk_t_train = get_feature_vectors(talk_data_train, 'talk_train', unique_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize `talk_data_test`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk_test_freq.csv successfully created!\n",
      "talk_test_binary.csv successfully created!\n",
      "talk_test_tfidf.csv successfully created!\n"
     ]
    }
   ],
   "source": [
    "talk_f_test, talk_b_test, talk_t_test = get_feature_vectors(talk_data_test, 'talk_test', unique_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
