{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 News group set feature extraction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "**This dataset is a collection of  20 newsgroups documents. The processing has been done for the purpose of feature extraction.**\n",
    "\n",
    "*This is a list of the 20 newsgroups:*\n",
    "\n",
    "- comp.graphics\n",
    "- comp.os.ms-windows.misc\n",
    "- comp.sys.ibm.pc.hardware\n",
    "- comp.sys.mac.hardware\n",
    "- comp.windows.x rec.autos\n",
    "- rec.motorcycles\n",
    "- rec.sport.baseball\n",
    "- rec.sport.hockey sci.crypt\n",
    "- sci.electronics\n",
    "- sci.med\n",
    "- sci.space\n",
    "- misc.forsale talk.politics.misc\n",
    "- talk.politics.guns\n",
    "- talk.politics.mideast talk.religion.misc\n",
    "- alt.atheism\n",
    "- soc.religion.christian\n",
    "\n",
    "#### Download Link: [20news-bydate.tar.gz ](http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz) - 20 Newsgroups sorted by date; duplicates and some headers removed (18846 documents)\n",
    "\n",
    "The 20 newsgroup dataset was transformed by using the **Bag of word** and **Term frequency-Inverse document frequency (tf-idf)** method. The dataset after transformation consists of five main classes:\n",
    "    \n",
    " - Computer\n",
    " - Recreational\n",
    " - Science\n",
    " - Talkshow\n",
    " - Other\n",
    " \n",
    " and each of these classes contains **`train.csv`** and **`test.csv`** files.\n",
    " \n",
    " \n",
    " **Used Methods**\n",
    " \n",
    " - [x] Bag of words - CountVectorizer\n",
    " - [x] Bag of Words - BinaryVectorizer\n",
    " - [x] TF-IDF - TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Generated CSV Files:\n",
    "\n",
    "`comp_count_train.csv` `comp_count_test.csv` `comp_binary_train.csv` `comp_binary_test.csv` `comp_tfidf_train.csv` `comp_tfidf_test.csv`\n",
    "\n",
    "`rec_count_train.csv` `rec_count_test.csv` `rec_binary_train.csv` `rec_binary_test.csv` `rec_tfidf_train.csv` `rec_tfidf_test.csv`\n",
    "\n",
    "`sci_count_train.csv` `sci_count_test.csv` `sci_binary_train.csv` `sci_binary_test.csv` `sci_tfidf_train.csv` `sci_tfidf_test.csv`\n",
    "\n",
    "`talk_count_train.csv` `talk_count_test.csv` `talk_binary_train.csv` `talk_binary_test.csv` `talk_tfidf_train.csv` `talk_tfidf_test.csv`\n",
    "\n",
    "`other_count_train.csv` `other_count_test.csv` `other_binary_train.csv` `other_binary_test.csv` `other_tfidf_train.csv` `other_tfidf_test.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules\n",
    "\n",
    "\n",
    "- The **OS module** in Python provides a way of using **operating system dependent functionality.** Here we use *os.walk* to go through all the files, directories and roots.\n",
    "- **Pandas** is a software library which use for **data manipulation and analysis**.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "\n",
    "# libraries importing from scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# natural language toolkit packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n",
    "from nltk.stem.snowball import  EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the set of stop words\n",
    "_stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "# initialize treebank word tokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# initialize treebank word detokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "# initialize english stemmer\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "# initialize word net lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# word list used in english language\n",
    "_word_list = set([word for word in wn.words(lang='eng')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get all the text files from the directory to a list\n",
    "def read_files(dir):\n",
    "    f = []\n",
    "    for roots, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            f.append(os.path.join(roots, file))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to categorize and read the files\n",
    "def open_files(file_paths):\n",
    "    comp = []\n",
    "    rec = []\n",
    "    sci = []\n",
    "    talk = []\n",
    "    other = []\n",
    "    \n",
    "    for path in file_paths:\n",
    "        if \"comp\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            comp += [w.read()]\n",
    "        elif \"rec\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            rec += [w.read()]\n",
    "        elif \"sci\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            sci += [w.read()]\n",
    "        elif \"talk\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            talk += [w.read()]\n",
    "        elif \"alt\" in path or \"misc\" in path or \"soc\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            other += [w.read()]\n",
    "    return comp, rec, sci, talk, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to clean data\n",
    "def clean_data(data):\n",
    "    cleaned_data = []\n",
    "    for text in data:\n",
    "        words = []\n",
    "        for word in tokenizer.tokenize(text):\n",
    "            word = stemmer.stem(word)\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            words += [word]   \n",
    "        cleaned_data += [detokenizer.detokenize([word for word in words if word in _word_list if word.isalpha()])]\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize_csv(cleaned_data, name):\n",
    "    vectorizer = CountVectorizer(stop_words=_stop_words)\n",
    "    x = vectorizer.fit_transform(cleaned_data)\n",
    "    y = vectorizer.get_feature_names()\n",
    "    \n",
    "    df = pd.DataFrame(data=x.toarray(), columns=y)\n",
    "    \n",
    "    train, test = train_test_split(df, train_size=0.7, test_size=0.3, random_state=42)\n",
    "    \n",
    "    train.to_csv(name+'_count_train.csv')\n",
    "    test.to_csv(name+'_count_test.csv')\n",
    "    \n",
    "    print('Count vectorize train and test csv for {0} is successfully created!'.format(name))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_vectorize_csv(cleaned_data, name):\n",
    "    vectorizer = CountVectorizer(stop_words=_stop_words, binary=True)\n",
    "    x = vectorizer.fit_transform(cleaned_data)\n",
    "    y = vectorizer.get_feature_names()\n",
    "    \n",
    "    df = pd.DataFrame(data=x.toarray(), columns=y)\n",
    "    \n",
    "    train, test = train_test_split(df, train_size=0.7, test_size=0.3, random_state=42)\n",
    "    \n",
    "    train.to_csv(name+'_binary_train.csv')\n",
    "    test.to_csv(name+'_binary_test.csv')\n",
    "    \n",
    "    print('Binary vectorize train and test csv for {0} is successfully created!'.format(name))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorize_csv(cleaned_data, name):\n",
    "    vectorizer = TfidfVectorizer(stop_words=_stop_words)\n",
    "    x = vectorizer.fit_transform(cleaned_data)\n",
    "    y = vectorizer.get_feature_names()\n",
    "    \n",
    "    df = pd.DataFrame(data=x.toarray(), columns=y)\n",
    "    \n",
    "    train, test = train_test_split(df, train_size=0.7, test_size=0.3, random_state=42)\n",
    "    \n",
    "    train.to_csv(name+'_tfidf_train.csv')\n",
    "    test.to_csv(name+'_tfidf_test.csv')\n",
    "    \n",
    "    print('Tfidf vectorize train and test csv for {0} is successfully created!'.format(name))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vectors(data, name):\n",
    "    cleaned_data = clean_data(data)\n",
    "    \n",
    "    c_train, c_test = count_vectorize_csv(cleaned_data, name)\n",
    "    b_train, b_test = binary_vectorize_csv(cleaned_data, name)\n",
    "    t_train, t_test = tfidf_vectorize_csv(cleaned_data, name)\n",
    "    return c_train, c_test, b_train, b_test, t_train, t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = read_files('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = []\n",
    "rec_data = []\n",
    "sci_data = []\n",
    "talk_data = []\n",
    "other_data = []\n",
    "\n",
    "comp_data, rec_data, sci_data, talk_data, other_data = open_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorize train and test csv for comp is successfully created!\n",
      "Binary vectorize train and test csv for comp is successfully created!\n",
      "Tfidf vectorize train and test csv for comp is successfully created!\n"
     ]
    }
   ],
   "source": [
    "comp_c_train, comp_c_test, comp_b_train, comp_b_test, comp_t_train, comp_t_test = get_feature_vectors(comp_data, 'comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorize train and test csv for rec is successfully created!\n",
      "Binary vectorize train and test csv for rec is successfully created!\n",
      "Tfidf vectorize train and test csv for rec is successfully created!\n"
     ]
    }
   ],
   "source": [
    "rec_c_train, rec_c_test, rec_b_train, rec_b_test, rec_t_train, rec_t_test = get_feature_vectors(rec_data, 'rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorize train and test csv for sci is successfully created!\n",
      "Binary vectorize train and test csv for sci is successfully created!\n",
      "Tfidf vectorize train and test csv for sci is successfully created!\n"
     ]
    }
   ],
   "source": [
    "sci_c_train, sci_c_test, sci_b_train, sci_b_test, sci_t_train, sci_t_test = get_feature_vectors(sci_data, 'sci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorize train and test csv for talk is successfully created!\n",
      "Binary vectorize train and test csv for talk is successfully created!\n",
      "Tfidf vectorize train and test csv for talk is successfully created!\n"
     ]
    }
   ],
   "source": [
    "talk_c_train, talk_c_test, talk_b_train, talk_b_test, talk_t_train, talk_t_test = get_feature_vectors(talk_data, 'talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorize train and test csv for other is successfully created!\n",
      "Binary vectorize train and test csv for other is successfully created!\n",
      "Tfidf vectorize train and test csv for other is successfully created!\n"
     ]
    }
   ],
   "source": [
    "other_c_train, other_c_test, other_b_train, other_b_test, other_t_train, other_t_test = get_feature_vectors(other_data, 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
